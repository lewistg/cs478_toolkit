\documentclass[12pt,letterpaper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{amssymb}
\usepackage{placeins}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[left=1.0in,top=1.0in,right=1.0in,bottom=1.0in]{geometry}
\usepackage{titling}
\setlength{\droptitle}{-4cm}
\renewcommand{\arraystretch}{1.1}
\usepackage{setspace}
\singlespacing

\title{Perceptron Project}
\author{Ty Lewis}
\begin{document} 
%\maketitle
%\vspace{-10pt}
\section*{Introduction}
The ``perceptron" is a simple machine learning algorithm capable of classifying... In this report we experiment with perceptron algorithm. This is done by, first, examining how well it labels linearly separable data versus data that is not linearly separable. Second, the perceptron is tested on 

\section*{Sensitivity to linear separability}

Sensitivity to linear separability was tested using training instances of two features $x \in [-1, 1]$ and $y \in [-1, 1]$. These $x$ and $y$ pairs were labeled either \emph{red} or \emph{blue}. One training set contained 4 red instances that were linearly separable from 4 blue instances. Another training set contained 4 red instances that were not linearly separable from the 4 blue instances. The perceptron trained on each set, stopping when 5 epochs had past without more than a 1 percent change in accuracy was observed between epochs.

\begin{wrapfigure}{r}{0.6\textwidth}
\vspace{-20pt}
\centering
\includegraphics[width=0.58\textwidth]{learning_rate_vs_epochs.eps} 
\vspace{-10pt}
\caption{Learning rate vs. epochs}
\vspace{-20pt}
\end{wrapfigure}

In one set of experiments, the effect of the learning rate on the amount of epochs it took to train was measured. The perceptron was trained using learning rates varying from $0.2$ to $2$. For each learning rate, 5 trials were run and the average number of epochs it took to finish training was measured. The graph in Figure 1 summarizes the results of this experiment.

\subsection*{Discussion of results}

A few features of this graph stand out. Note that for the linearly separable data, the number of epochs it took to train was relatively small. Conversely, it took much longer to train on the non-linearly separable data. It also appears that there is a high amount of variance in the non-linearly separable data. Indeed, for some learning rate experiments, the amount of epochs it took to train varied from as low as 9 all the way up to 80 or from 13 to 124. This was not the case for the linearly separable data. Overall, it doesn't appear that the number of epochs is very affected by this range of learning rates.


To see the For a learning rate of $0.1$, the following results were obtained.

\begin{center}
\begin{figure}[H]
	\begin{subfigure}[h]{0.5\textwidth}
		\includegraphics[scale=0.40]{linsep.eps} 
	\end{subfigure}
	\begin{subfigure}[h]{0.5\textwidth}
		\includegraphics[scale=0.40]{nonlinsep.eps} 
	\end{subfigure}
\caption{Results of perceptron and linearly separable and non-linearly separable data.}
\end{figure}
\end{center}

\section*{Voting task}

In this learning task, the perceptron seeks to identify Democrats and Republicans based on their stance on various issues. The table to the belows summarizes the performance of the perceptron for five trials. Note that in each trial the data was randomly split 70\% training and 30\% for testing.

%\begin{wrapfigure}{r}{0.6\textwidth}
\begin{figure}[H]
\centering
\begin{tabular}{cccc}
\toprule
Trial	&	Training set accuracy	&	Test set accuracy 	&	Epochs to train\\
\hline
1	&	96.9	&	95.7	&	37\\
2	&	94.7	&	95.7	&	46\\
3	&	97.5	&	94.9	&	19\\
4	&	97.8	&	94.2	&	86\\
5	&	97.8	&	90.6	&	10\\
\hline
Average	&	96.94	&	94.22	&	39.6\\
\bottomrule
\end{tabular}
\caption{Accuracy of perceptron trained and tested for voting task}
\end{figure}
%\end{wrapfigure}

Below is a graph depicting how the average error rate for the five trials changed after each epoch. Notice that after just a few epochs, the average error rate is already very low. The graph is also very sporadic. This may be due to the fact that trial 4 took much longer to train on than the others, essentially the average error rate consists of only trial 4's data beyond the $46$th epoch. Despite the sporadic data behavior, it is somewhat discernible that there is a general downward trend as the epoch continue.

\bigskip

%\begin{center}
%\begin{figure}[H]
\begin{wrapfigure}{r}{0.6\textwidth}
\centering
\includegraphics[width=0.55\textwidth]{error.pdf} 
\caption{Learning rate vs. epochs}
\vspace{-30pt}
\end{wrapfigure}
%\end{figure}
%\end{center}

For each trial, the final weights were extracted from the perceptron's model. The average of these weights is shown in the table below. Note that $w_i$ refers to the $i$th feature in the attributes section of the \emph{voting.arff} file (excluding the label feature). The last weight, $w_{16}$ is the bias weight.

\subsection*{Discussion of model}

The perceptron outputs a 0 if the feature pattern indicates a Democrat and a 1 for Republican. In other words the net-value is negative for Democrats and positive for Republicans. Notice that the largest negative weights are $w_3, w_4$, and $w_9$ which respectively refer to ``physician-fee-freeze", ``el-salvador-aide", and ``immigration." We can take these large negative weights to mean that these are the most significant indicators of a Democrat. Alternatively, $w_{10}, w_2$, and $w_8$ are the largest positive weights. They correspond to ``synfuels-corporation-cutback", ``adoption-of-the-budget-resolution", and ``mx-missile" respectively. Similarly, these are the most significant predictors of a Republican. 

\begin{figure}[h]
\centering

\begin{tabular}{c|c||c|c||c|c||c|c||c|c}
\toprule
\multicolumn{10}{c}{Voting Task Model} \\
\hline
$w_{0}$	&	0.16	&	$w_{4}$	&	-0.66	&	$w_{8}$	&	0.76	&	$w_{12}$	&	-0.1	&	$w_{16}$	&	1.42\\
$w_{1}$	&	0.3	&	$w_{5}$	&	0.14	&	$w_{9}$	&	-0.54	&	$w_{13}$	&	0.12	&		&	\\
$w_{2}$	&	0.76	&	$w_{6}$	&	-0.4	&	$w_{10}$	&	1.04	&	$w_{14}$	&	0.48	&		&	\\
$w_{3}$	&	-1.88	&	$w_{7}$	&	-0.8	&	$w_{11}$	&	-0.02	&	$w_{15}$	&	-0.26	&		&	\\
\bottomrule
\end{tabular}
\caption{Average weights for perceptron model after training on voting task.}
\end{figure}

\section*{Iris task}

\textbf{Note:} For the iris learning task, training is stopped once 100 epochs have passed without a significant improvement in the best model found so far. (Using the stopping criteria from previous experiments often never halted training; the training set accuracy simply thrashed from as low as 60\% to as high as 90\%, never converging.)

Below is the table showing accuracy of the model for the training set and the test set.

\begin{center}
\begin{tabular}{cccc}
\toprule
Trial	&	Training set accuracy	&	Test set accuracy 	&	Epochs to train\\
\hline
1	&	95.2	&	95.6	&	121\\
2	&	95.2	&	88.2	&	110\\
3	&	95.2	&	93.3	&	164\\
4	&	96.2	&	91.1	&	191\\
5	&	95.2	&	97.8	&	106\\
Average	&	95.4	&	93.2	&	138.4\\
\bottomrule
\end{tabular}
\end{center}

For this data set there were three possible output classes that could be assigned to each instance, namely iris-setosa, iris-versicolor, and iris-virginica. In order to deal with this, a perceptron was created for flower type and trained to identify only that type. The table below shows the average weights for each perceptron. Again, the $i$th weight corresponds to the $i$th feature in the \emph{iris.arff} file (excluding the label attribute) and $w_5$ is the bias weight.

\begin{center}
\begin{tabular}{lccccc}
\toprule
\multicolumn{6}{c}{Iris Task Models} \\
\hline
Flower Modeled	&	$w_0$	&	$w_1$	&	$w_2$	&	$w_4$	&	$w_5$\\
\hline
Iris-setosa	&	0.158	&	0.514	&	-0.8	&	-0.386	&	0.1\\
Iris-versicolor	&	1.114	&	-3.714	&	2.006	&	-4.922	&	3.18\\
Iris-virginica	&	-2.426	&	-2.716	&	3.974	&	3.816	&	-1.58\\
\bottomrule
\end{tabular}
\end{center}

\subsection*{Discussion of model}

Considering the weights of largest magnitude for each flower in the model above. It appears that the model has learned that the most indicative features for classifying the iris-setosa is sepal width and petal length and for iris-versicolor petal length and petal width seem most important. Interestingly, for iris-virginica the model has given approximately equal weight to sepal length and sepal width for determining a flower is not an iris-virginica, and it it has also given approximately equal weights to petal length and petal width for determining a flower is an iris-virginica.

\section*{My own experiment}

For my own experiment I worked with the data set pictured in the graph below. This data set consists of $x$ $y$ pairs of points labeled either \emph{red} or \emph{blue}. Notice that the blue points are randomly distributed around the green points and the red points are randomly distributed around the yellow points. The green and yellow points were also randomly distributed.

One way that we can try to linearly separate these points is by performing first performing a non-linear transformation on these points. We will use the following non-linear transformation.
$$\Phi(x,y,n) = (\sum\limits_{i=0}^n \sum\limits_{j=0}^nx^iy^j) - 1 = x^0y^1 + x^0y^2 \cdots x^1y^0 + x^1y^1 + x^1y^2 \cdots x^ny^{n-1} + x^ny^n = a_0 + a_1 \cdots a_n$$
Where $n$ is the degree of polynomial and $a_i$ denotes the $i$th term in the polynomial. After performing this transformation, the terms $a_i$ can be used as features in an \emph{arff} file, and the perceptron can be used to learn the weights $w_i$ in the polynomial
$$f(x,y) = w_ia_i + w_na_n + w_{n+1}$$
where $w_{n+1}$ is the bias weight. In this experiment, I try to answer the following question: Which values of $n$ result in polynomials that fit the data above? From this, I hope to infer whether or not there is a point of ``diminishing returns" where any further explosion of the data fails to give better results. 

\end{document}