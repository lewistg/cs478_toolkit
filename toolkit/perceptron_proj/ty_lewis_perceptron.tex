\documentclass[12pt,letterpaper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[left=1.0in,top=1.0in,right=1.0in,bottom=1.0in]{geometry}
\renewcommand{\arraystretch}{1.1}

\title{Perceptron Project}
\author{Ty Lewis}
\begin{document} 
\maketitle
\section*{Introduction}
The ``perceptron" is a simple machine learning algorithm capable of classifying... In this report we experiment with perceptron algorithm. This is done by, first, examining how well it labels linearly separable data versus data that is not linearly separable. Second, the perceptron is tested on 

\section*{Sensitivity to linear separability}

Sensitivity to linear separability was tested using training instances of two features $x \in [-1, 1]$ and $y \in [-1, 1]$. These $x$ and $y$ pairs were labeled either \emph{red} or \emph{blue}. One training set contained 4 red instances that were linearly separable from 4 blue instances. Another training set contained 4 red instances that were not linearly separable from the 4 blue instances. The perceptron trained on each set, stopping when 5 epochs had past without more than a 1 percent change in accuracy was observed between epochs.

In one set of experiments, the effect of the learning rate on the amount of epochs it took to train was measured. The perceptron was trained using learning rates varying from $0.2$ to $2$. For each learning rate, 5 trials were run and the average number of epochs it took to finish training was measured. The graph in Figure 1 summarizes the results of this experiment.

\subsection*{Discussion of results}

A few features of this graph stand out. Note that for the linearly separable data, the number of epochs it took to train was relatively small. Conversely, it took much longer to train on the non-linearly separable data. It also appears that there is a high amount of variance in the non-linearly separable data. Indeed, for some learning rate experiments, the amount of epochs it took to train varied from as low as 9 all the way up to 80 or from 13 to 124. This was not the case for the linearly separable data. Overall, it doesn't appear that the number of epochs is very affected by this range of learning rates.

\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.5]{learning_rate_vs_epochs.eps} 
\end{center}
\caption{Graph that shows the effect of learning rate upon the amount of epochs it takes to train.}
\end{figure}

To see the For a learning rate of $0.1$, the following results were obtained.

\begin{center}
\begin{figure}[H]
	\begin{subfigure}[h]{0.5\textwidth}
		\includegraphics[scale=0.40]{linsep.eps} 
	\end{subfigure}
	\begin{subfigure}[h]{0.5\textwidth}
		\includegraphics[scale=0.40]{nonlinsep.eps} 
	\end{subfigure}
\caption{Results of perceptron and linearly separable and non-linearly separable data.}
\end{figure}
\end{center}

\section*{Voting task}

\begin{tabular}{cccc}
\toprule
Trial	&	Training set accuracy	&	Test set accuracy 	&	Epochs to train\\
\hline
1	&	96.9	&	95.7	&	37\\
2	&	94.7	&	95.7	&	46\\
3	&	97.5	&	94.9	&	19\\
4	&	97.8	&	94.2	&	86\\
5	&	97.8	&	90.6	&	10\\
\hline
Average	&	96.94	&	94.22	&	39.6\\
\bottomrule
\end{tabular}


\begin{tabular}{ccccccc}
\toprule
Trial	&	1	&	2	&	3	&	4	&	5	&	Average\\
\hline
$w_0$	&	0.1	&	0.1	&	0.3	&	0.2	&	0.1	&	0.16\\
$w_1$	&	0.4	&	0.2	&	0.3	&	0.5	&	0.1	&	0.3\\
$w_2$	&	1.1	&	0.7	&	0.5	&	1	&	0.5	&	0.76\\
$w_3$	&	-1.9	&	-1.5	&	-1.7	&	-2.5	&	-1.8	&	-1.88\\
$w_4$	&	-0.7	&	-0.6	&	-0.2	&	-1.4	&	-0.4	&	-0.66\\
$w_5$	&	2.78E-017	&	0.2	&	0.3	&	2.78E-017	&	0.2	&	1.40E-001\\
$w_6$	&	-0.1	&	-0.2	&	-0.3	&	-1.1	&	-0.3	&	-0.4\\
$w_7$	&	-0.8	&	-0.8	&	-0.9	&	-1.3	&	-0.2	&	-0.8\\
$w_8$	&	0.6	&	0.5	&	1	&	1	&	0.7	&	0.76\\
$w_9$	&	-0.5	&	-0.4	&	-0.6	&	-0.8	&	-0.4	&	-0.54\\
$w_{10}$	&	0.9	&	0.9	&	1	&	1.1	&	1.3	&	1.04\\
$w_{11}$	&	-2.78E-017	&	-0.1	&	-0.3	&	-2.78E-017	&	0.3	&	-2.00E-002\\
$w_{12}$	&	-0.3	&	0.1	&	-0.4	&	-0.1	&	0.2	&	-0.1\\
$w_{13}$	&	0.2	&	0.1	&	0.1	&	0.4	&	-0.2	&	0.12\\
$w_{14}$	&	0.7	&	0.7	&	0.7	&	0.2	&	0.1	&	0.48\\
$w_{15}$	&	-0.5	&	-0.2	&	0.1	&	-0.4	&	-0.3	&	-0.26\\
$w_{16}$	&	1.6	&	1.2	&	0.9	&	2.6	&	0.8	&	1.42\\
\bottomrule
\end{tabular}



\section*{Iris task}

Note that for this learning task training is stopped once 100 epochs have passed without a significant improvement in the best model found so far. Recall, that for previous experiments  

\end{document}